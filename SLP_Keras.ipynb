{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66d141c9-41f6-459e-aba9-c86f10b88d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeac8062-f555-4acb-852f-fb158b752f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd0a69f-a67d-4b6f-8b64-eae9800ff32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ccheck if GPU is actually used for execution (mainly for model trraining and stuff)\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426c7eaa-0095-4aa5-afc2-860e1733bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train), (x_test,y_test) = keras.datasets.mnist.load_data()\n",
    "# The train set (x_train) contains 60k images each of it is a matrix of size 28*28\n",
    "# Each image is 28x28 pixels\n",
    "# Each pixel ranges from 0 to 255\n",
    "# Test set has 10k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5580a285-5111-4a91-ace2-8a958da91a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x106081de0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(x_train[0].shape)\n",
    "plt.matshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef0a15a1-5b25-4964-891b-c681325e3b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c11998-db5e-412d-af39-3df290d87781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we normalize the dataset to make calculations fast and accurate.\n",
    "# NN expects a flat 1D input, so flatten the 2D 28x28 image into 784-lenght vectors\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# Flatten the dataset\n",
    "x_train_flatten = x_train.reshape(len(x_train),28*28)\n",
    "x_test_flatten = x_test.reshape(len(x_test),28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c54bd49-a062-4cdf-b9ec-7d37d35939d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed400784-5d19-4aa6-825d-726b33255f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fe03e66-a17c-4e58-b91c-cff3643d1320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.50196078, 0.50196078, 1.        , 1.        ,\n",
       "       1.        , 0.74901961, 0.25098039, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.50196078, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.50196078, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.74901961,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.50196078, 0.25098039, 0.25098039, 0.25098039,\n",
       "       1.        , 1.        , 1.        , 0.25098039, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.25098039, 0.74901961, 1.        ,\n",
       "       1.        , 1.        , 0.25098039, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.50196078, 1.        , 1.        , 1.        , 0.25098039,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.25098039, 1.        , 1.        , 1.        , 0.50196078,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.50196078, 1.        ,\n",
       "       1.        , 1.        , 0.74901961, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.25098039, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.74901961, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.25098039, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.50196078, 0.50196078,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.25098039, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.25098039, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.74901961, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.74901961,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.25098039, 0.50196078, 0.74901961, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.50196078, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.25098039,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.74901961,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.74901961, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.74901961,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.50196078,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.25098039, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.50196078, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.25098039,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.50196078, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.74901961,\n",
       "       1.        , 1.        , 1.        , 0.50196078, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.50196078, 1.        , 1.        ,\n",
       "       0.74901961, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.25098039, 1.        , 1.        , 0.50196078, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flatten[5999] #x_train_flatten is now an array of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cdb77-d6f1-44d8-b0bc-fb62f7dbc62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d76dfda4-af02-4f19-b6fd-6afa03790c01",
   "metadata": {},
   "source": [
    "## Build a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9529cedc-7d56-4fc5-b4bc-0a000d6f5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with a sinlge layer perceptron. One input layer and one output layer, no hidden layer\n",
    "# 10 output neurons (one for each digit from 0 to 9)\n",
    "# input_shape = 784 matches the flattened image shape\n",
    "# uses sigmoid activation function as it returns probabilites b/w 0 and 1\n",
    "# Dense Layer (Output Layer)\n",
    "    # Dense(10) → this is the only trainable layer (i.e., with weights).\n",
    "    # It has:\n",
    "        # 784 weights per neuron × 10 neurons = 7,840 weights\n",
    "        # 10 biases (1 per neuron)\n",
    "        # Activation: sigmoid applied to each output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "290fc862-625e-47d6-8832-7c86b4becddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERY IMP\n",
    "# weights adjusted not using the basic perceptron learning rule — \n",
    "# instead, we are using gradient-based optimization (Adam + cross-entropy).\n",
    "# Weights adjusted using backpropogation, not classic perceptron rul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b30279aa-f726-42b0-965e-89c8026d1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - accuracy: 0.7534 - loss: 0.9685\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9027 - loss: 0.3560\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - accuracy: 0.9123 - loss: 0.3155\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - accuracy: 0.9200 - loss: 0.2885\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.9204 - loss: 0.2854\n",
      "Epoch 6/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.9249 - loss: 0.2701\n",
      "Epoch 7/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9261 - loss: 0.2666\n",
      "Epoch 8/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - accuracy: 0.9259 - loss: 0.2638\n",
      "Epoch 9/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - accuracy: 0.9274 - loss: 0.2608\n",
      "Epoch 10/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9265 - loss: 0.2618\n",
      "Epoch 11/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - accuracy: 0.9288 - loss: 0.2539\n",
      "Epoch 12/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9304 - loss: 0.2540\n",
      "Epoch 13/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - accuracy: 0.9299 - loss: 0.2486\n",
      "Epoch 14/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - accuracy: 0.9310 - loss: 0.2539\n",
      "Epoch 15/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9322 - loss: 0.2494\n",
      "Epoch 16/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.9333 - loss: 0.2413\n",
      "Epoch 17/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9314 - loss: 0.2422\n",
      "Epoch 18/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - accuracy: 0.9332 - loss: 0.2420\n",
      "Epoch 19/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - accuracy: 0.9328 - loss: 0.2424\n",
      "Epoch 20/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - accuracy: 0.9325 - loss: 0.2444\n",
      "Epoch 21/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - accuracy: 0.9332 - loss: 0.2448\n",
      "Epoch 22/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9342 - loss: 0.2405\n",
      "Epoch 23/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.9347 - loss: 0.2403\n",
      "Epoch 24/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - accuracy: 0.9341 - loss: 0.2402\n",
      "Epoch 25/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.9344 - loss: 0.2374\n",
      "Epoch 26/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - accuracy: 0.9330 - loss: 0.2417\n",
      "Epoch 27/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - accuracy: 0.9337 - loss: 0.2388\n",
      "Epoch 28/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9339 - loss: 0.2377\n",
      "Epoch 29/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.9353 - loss: 0.2346\n",
      "Epoch 30/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - accuracy: 0.9346 - loss: 0.2381\n",
      "Epoch 31/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - accuracy: 0.9353 - loss: 0.2336\n",
      "Epoch 32/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.9339 - loss: 0.2359\n",
      "Epoch 33/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9345 - loss: 0.2393\n",
      "Epoch 34/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.9373 - loss: 0.2288\n",
      "Epoch 35/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.9353 - loss: 0.2364\n",
      "Epoch 36/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - accuracy: 0.9325 - loss: 0.2367\n",
      "Epoch 37/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - accuracy: 0.9356 - loss: 0.2355\n",
      "Epoch 38/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9364 - loss: 0.2304\n",
      "Epoch 39/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.9362 - loss: 0.2327\n",
      "Epoch 40/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - accuracy: 0.9365 - loss: 0.2328\n",
      "Epoch 41/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.9379 - loss: 0.2254\n",
      "Epoch 42/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.9374 - loss: 0.2251\n",
      "Epoch 43/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.9373 - loss: 0.2338\n",
      "Epoch 44/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9353 - loss: 0.2387\n",
      "Epoch 45/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step - accuracy: 0.9376 - loss: 0.2238\n",
      "Epoch 46/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.9374 - loss: 0.2239\n",
      "Epoch 47/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9377 - loss: 0.2243\n",
      "Epoch 48/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - accuracy: 0.9373 - loss: 0.2267\n",
      "Epoch 49/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.9372 - loss: 0.2251\n",
      "Epoch 50/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - accuracy: 0.9375 - loss: 0.2233\n",
      "CPU training time: 11.50 seconds\n"
     ]
    }
   ],
   "source": [
    "# Sequential groups a linear stack of layers into a Model.\n",
    "with tf.device('/CPU:0'):\n",
    "    # keras.layers.Dense(10, input_shape=(784,),\n",
    "                       # activation='sigmoid')\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(784,)),\n",
    "        keras.layers.Dense(10, activation='sigmoid')\n",
    "    ])\n",
    "    # .compile configures the model for training.\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy'])\n",
    "    start = time.time()\n",
    "    # .fit Trains the model for a fixed number of epochs\n",
    "    model.fit(x_train_flatten, y_train, epochs = 50, batch_size=100,verbose=1)\n",
    "    end=time.time()\n",
    "    print(f\"CPU training time: {end-start:.2f} seconds\")\n",
    "# batch_size = Number of samples processed before the model updates weights once.\n",
    "# model takes a batch of 100 samples at once, calcualtes combined batch loss from individual sample loss \n",
    "# and using this batch loss, and using backpropogation and gradient descent, the weights are updated\n",
    "# SO weights updated once per batch in an epoch\n",
    "# USe batch training for parallelisms\n",
    "\n",
    "# verbose = Controls how much information Keras prints during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7e7eaf5-a106-4e85-a665-0b4d24e8690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7466 - loss: 0.9807\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9021 - loss: 0.3619\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9133 - loss: 0.3145\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9174 - loss: 0.2930\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9213 - loss: 0.2801\n",
      "Epoch 6/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9217 - loss: 0.2764\n",
      "Epoch 7/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9236 - loss: 0.2732\n",
      "Epoch 8/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9264 - loss: 0.2620\n",
      "Epoch 9/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9267 - loss: 0.2627\n",
      "Epoch 10/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9280 - loss: 0.2549\n",
      "Epoch 11/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9300 - loss: 0.2559\n",
      "Epoch 12/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9300 - loss: 0.2558\n",
      "Epoch 13/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9289 - loss: 0.2534\n",
      "Epoch 14/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9309 - loss: 0.2531\n",
      "Epoch 15/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9294 - loss: 0.2558\n",
      "Epoch 16/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9331 - loss: 0.2434\n",
      "Epoch 17/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9324 - loss: 0.2436\n",
      "Epoch 18/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.2437\n",
      "Epoch 19/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9328 - loss: 0.2432\n",
      "Epoch 20/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9340 - loss: 0.2438\n",
      "Epoch 21/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9320 - loss: 0.2413\n",
      "Epoch 22/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9327 - loss: 0.2394\n",
      "Epoch 23/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9327 - loss: 0.2455\n",
      "Epoch 24/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9342 - loss: 0.2389\n",
      "Epoch 25/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9335 - loss: 0.2415\n",
      "Epoch 26/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9351 - loss: 0.2353\n",
      "Epoch 27/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9346 - loss: 0.2332\n",
      "Epoch 28/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9357 - loss: 0.2375\n",
      "Epoch 29/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9339 - loss: 0.2467\n",
      "Epoch 30/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.2376\n",
      "Epoch 31/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9370 - loss: 0.2276\n",
      "Epoch 32/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9354 - loss: 0.2380\n",
      "Epoch 33/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9346 - loss: 0.2386\n",
      "Epoch 34/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9372 - loss: 0.2269\n",
      "Epoch 35/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9349 - loss: 0.2332\n",
      "Epoch 36/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9354 - loss: 0.2328\n",
      "Epoch 37/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9373 - loss: 0.2330\n",
      "Epoch 38/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9360 - loss: 0.2331\n",
      "Epoch 39/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9344 - loss: 0.2369\n",
      "Epoch 40/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9358 - loss: 0.2346\n",
      "Epoch 41/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9358 - loss: 0.2342\n",
      "Epoch 42/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9371 - loss: 0.2285\n",
      "Epoch 43/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9369 - loss: 0.2236\n",
      "Epoch 44/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9360 - loss: 0.2270\n",
      "Epoch 45/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9366 - loss: 0.2303\n",
      "Epoch 46/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9385 - loss: 0.2258\n",
      "Epoch 47/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9362 - loss: 0.2332\n",
      "Epoch 48/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9367 - loss: 0.2287\n",
      "Epoch 49/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9364 - loss: 0.2264\n",
      "Epoch 50/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9360 - loss: 0.2285\n",
      "CPU training time: 90.68 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train using GPU\n",
    "# GPU training took longer time as it is a very simple model and not enough compute to benfit from GPU\n",
    "# also Apple GPUs not optimized for parallel dense matrix math\n",
    "# overhead of moving data to GPU outweighs the speedup\n",
    "with tf.device('/GPU:0'):\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(784,)),\n",
    "        keras.layers.Dense(10, activation='sigmoid')\n",
    "    ])\n",
    "    # .compile configures the model for training.\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy'])\n",
    "    start = time.time()\n",
    "    # .fit Trains the model for a fixed number of epochs\n",
    "    model.fit(x_train_flatten, y_train, epochs = 50, batch_size=100,verbose=1)\n",
    "    end=time.time()\n",
    "    print(f\"CPU training time: {end-start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32da5160-f0cf-4264-8edd-e0ed742d8434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.models.sequential.Sequential"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8a5f4-a96e-4402-9961-88ba86f380a3",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "40bf6947-b4f8-4f41-a96b-28bb21d66908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9164 - loss: 0.3046\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy= model.evaluate(x_test_flatten, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f2a81663-8e71-41f0-8626-64e5568f790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.2696640193462372 and accurcacy is 0.9275000095367432\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss is {loss} and accurcacy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8dddb0ec-f048-4be4-bca2-e14d8b3483b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "single_input = x_test[80]         # shape: (28, 28)\n",
    "flattened_input = single_input.reshape(1, 784)  # reshape to (1, 784)\n",
    "\n",
    "# Normalize if your training data was normalized (e.g. /255)\n",
    "# No need to again normalize as x_test is already normalised\n",
    "# flattened_input = flattened_input / 255.0\n",
    "\n",
    "# Predict probabilities\n",
    "pred_probs = model.predict(flattened_input)  # shape: (1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a2d071aa-3f79-434a-89b0-efe889dafa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1035418e-05, 3.1003388e-08, 5.6303607e-04, 4.5931441e-01,\n",
       "        2.4099877e-01, 4.3272087e-01, 4.2419122e-09, 9.9493635e-01,\n",
       "        1.9860305e-02, 9.8832393e-01]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "82d21ee2-e309-4fc1-9e7c-eb10d563e7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x394d1d540>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_test[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "62d80d02-5443-4e0e-9f1b-2c8e36fff449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5b3ae-e4b0-4354-a04b-85af15da449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refered: https://www.geeksforgeeks.org/single-layer-perceptron-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c5d6f-d4a2-43fa-a45b-f17c0b114ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
